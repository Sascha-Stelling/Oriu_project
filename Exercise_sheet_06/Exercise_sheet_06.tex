\documentclass[8pt,a4]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{hyperref}

\author{Shivali Dubey, Sascha Stelling}
\title{Object Recognition and Image Understanding Exercise Sheet 6}

\begin{document}

\maketitle

\paragraph{Question 1}
\begin{itemize}
	\item \textbf{Team:} \\Shivali:
	\begin{itemize}
		\item Defining PReLU function
		\item Training of datasets (backpropagation)
		\item testing (result from Euclidean loss objective function)
		\item Reducing overfitting
	\end{itemize}
	Sascha:
	\begin{itemize}
		\item Convolution filters
		\item Training of dataset (feed forward)
		\item Testing (results from multinomial logistic regression)
	\end{itemize}
	\item \textbf{Problem Definition:} \textit{Object detection and image classification} has various applications, for instance, in self-driving cars to detect and classify pedestrians, motorcycles, trees, bicycles etc; classification of features on the Earth such as roads, rivers, agricultural fields etc using satellite images. With the advancements in deep learning, every year, new algorithms/ models keep on outperforming the previous ones, to achieve the best possible accuracies for image classification. One of the most popular dataset used is the ImageNet dataset. In our project we propose to implement the deep learning algorithms based on a few selected studies\footnote{Girshick, R., Donahue, J., Darrell, T., Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. Tech Report (V5). UC Berkeley. October, 2014.}\footnote{He, K., Zhang, X., Ren, S., Sun, J. Delving Deep into Rectifiers: Surpassing Human-level Performance on ImageNet Classification. Microsoft Research. February, 2015.}\footnote{Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106–1114, 2012.
	} with the aim to attain the best possible classification accuracy.
	
	\item \textbf{Dataset:} Tiny ImageNet\footnote{\url{https://tiny-imagenet.herokuapp.com/}}. Tiny Imagenet has 200 classes. Each class has 500 training images, 50 validation images, and 50 test images.
	
	\item \textbf{Approach:} With such a large dataset, one of the main challenges of classification is diversity of the images. Our model/algorithm must be able to handle fine-grained and specific classes even when they are hard to distinguish. In other words, we need to maximize inter-class variability, while minimize intra-class variability. At the same time, attaining the best possible classification accuracy is always a challenge for any given algorithm. The predictions go wrong when you have too many false positives and false negatives.
	
	Image Classification:
	\\Architecture:
	\begin{itemize}
		\item Convolution: The main purpose of using multiple convolution layers is feature extraction. We use Scale-Invariant Feature Transform (SIFT)\footnote{\url{https://pdfs.semanticscholar.org/presentation/e903/196678c93315f2bf6f0235b3bab59c157b04.pdf}} descriptors which computes the Difference of Gaussians (DoG)\footnote{\url{http://micro.magnet.fsu.edu/primer/java/digitalimaging/processing/diffgaussians/index.html}}. DoG is used to detect blobs by subtracting two blurred images from another with different Gaussian kernels. The maxima and minima of this operation are taken by SIFT as key feature locations for the next neurons. To classify feature more accurately we make use of densely sampled SIFT, Extended Opponent SIFT and RGB-SIFT detector as described in \footnote{Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106–1114, 2012.
		} in three different convolution layers. We use Parametric ReLU (PReLU)\footnote{\url{https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf?spm=5176.100239.blogcont55892.28.pm8zm1&file=He_Delving_Deep_into_ICCV_2015_paper.pdf}} for activation of neuron and Adam\footnote{\url{https://arxiv.org/pdf/1412.6980.pdf}} for optimization. PReLU is being used instead of ReLU because it improves model fitting with nearly zero computational cost and little overfitting risk. Also, PReLU (and also ReLU) brings non-linearity into the system which allows learning complex functions. The weight initialization can be performed using Xavier's initialization\footnote{\url{http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}}. Furthermore, the weight optimization is also controlled by PReLU.
		\item Downsampling:  Though the activation function passes only relevant pixels to the next layer, the array could still be big. To reduce the size of the array, we downsample it using an algorithm called max pooling.
		\item Fully-connected Neural Network: We construct the last layer as fully connected neural network with hidden layer and logistic regression, and set all feature maps produced from previous layers as inputs. Softmax logistic regression can be used to represent categorical distribution i.e. probability distribution over different outcomes.
		\item Backpropagation: We use gradient descent method for optimization algorithm which is thus used for learning and training.
	\end{itemize}
	
	\textbf{Reducing Overfitting:} As studied in previous research works (Imagenet class), we perform data augmentation and dropout to reduce overfitting. Data augmentation refers to artificially enlarging image size using augmentation. Dropout refers to dropping out the output of each hidden neuron with probability 0.5 and less, such that the respective neuron can't participate in backpropagation\footnote{Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014.
	}. 
	\\\textbf{Training:} The image is first cropped representing an object part or a small object. The training is carried out by optimising the multinomial Euclidian loss (for bounding box representation, otherwise regression is more common) objective function using mini-batch gradient-descent (based on back-propagation)\footnote{Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106–1114, 2012.}\footnote{LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.
	}\footnote{Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014.}. 
	\\\textbf{Testing:} At test time, we have a trained dataset and an input image. The fully connected layers are converted to convolutional layers resulting into a Fully Convolutional (FC) network. During training, the images are cropped, however during testing the FC network is applied on the entire image. The result is a bounding box location prediction\footnote{Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106–1114, 2012.} 5].
	Note: We could also train our dataset by optimising the multinomial logistic regression objective function. In this case, the testing would result in a class score map with the number of channels equal to the number of classes\footnote{LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.}\footnote{Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014.}.
	Results and Evaluation: We expect our output images to be classified and represented with box bounded objects. The results can be quantitatively evaluated by calculating the top 1 and top 5 test set error rates as done in previous studied\footnote{Girshick, R., Donahue, J., Darrell, T., Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. Tech Report (V5). UC Berkeley. October, 2014.}\footnote{He, K., Zhang, X., Ren, S., Sun, J. Delving Deep into Rectifiers: Surpassing Human-level Performance on ImageNet Classification. Microsoft Research. February, 2015.}\footnote{Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106–1114, 2012.}5]. The qualitative evaluation can be performed by selecting 5 most probable class for a given object in an image and computing their probabilities. 
	
	
	\item \textbf{Evaluation $\mathbf{\&}$ Expected  Results:} We expect our output images to be classified and represented with box bounded objects. The results can be quantitatively evaluated by calculating the top 1 and top 5 test set error rates as done in previous studied\footnote{Girshick, R., Donahue, J., Darrell, T., Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. Tech Report (V5). UC Berkeley. October, 2014.
	}\footnote{He, K., Zhang, X., Ren, S., Sun, J. Delving Deep into Rectifiers: Surpassing Human-level Performance on ImageNet Classification. Microsoft Research. February, 2015.
}\footnote{Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106–1114, 2012.
}\footnote{
Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014.
}. The qualitative evaluation can be performed by selecting 5 most probable class for a given object in an image and computing their probabilities. 
	
	\item \textbf{Hardware:} We plan to use our own PCs for this task having the following specifications:
	Intel Core i5-3350P @ 3.1GHz, 12GB DDR3, Nvidia GTX 970, 128GB SSD
	
	\item \textbf{Excluded Presentation Date:} None
\end{itemize}

\end{document}